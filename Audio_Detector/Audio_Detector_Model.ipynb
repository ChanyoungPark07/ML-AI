{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccca7f8-9049-48ba-bc01-770f2c1ea7b5",
   "metadata": {},
   "source": [
    "# Audio Detector Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010011fc-0690-4f53-9eab-8a696e1c6328",
   "metadata": {},
   "source": [
    "##### Dataset downloaded from Kaggle: https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4e2f4-e0c1-4265-ba71-a608536f8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import librosa.display\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaee0b7-15a1-4c97-84e9-836db4a0e84a",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8f566-93d5-41f3-98cb-d702f13e01c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get audio paths and store them in a dataframe\n",
    "folder_path = os.getcwd() + '/recordings'\n",
    "speaker_paths = [os.path.join(folder_path, speaker_folder) \\\n",
    "                 for speaker_folder in os.listdir(folder_path) \\\n",
    "                 if not speaker_folder.startswith('.')]\n",
    "\n",
    "audio_paths = [os.path.join(speaker_path, audio_file) \\\n",
    "               for speaker_path in speaker_paths \\\n",
    "               for audio_file in os.listdir(speaker_path)]\n",
    "\n",
    "audio_df = pd.DataFrame({'audio_path': audio_paths})\n",
    "audio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c34929-013f-4323-8471-550c81236dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign speaker and audio label to dataframe\n",
    "\n",
    "def extract_speaker(path):\n",
    "    return path.split('/')[-1].split('_')[1]\n",
    "\n",
    "def extract_labels(path):\n",
    "    return path.split('/')[-1].split('_')[0]\n",
    "\n",
    "audio_df['speaker_label'] = audio_df.get('audio_path').apply(extract_speaker)\n",
    "audio_df['audio_label'] = audio_df.get('audio_path').apply(extract_labels)\n",
    "\n",
    "audio_df = audio_df.sort_values(by='audio_label')\n",
    "audio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258485d-ef34-4e4b-897e-98cb8b90bc16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert spectrogram data into tensors and save dataframe as a csv\n",
    "\n",
    "def spectrogram_to_tensor(path):\n",
    "    audio, sr = librosa.load(path)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    spectrogram /= np.max(spectrogram)\n",
    "    return tf.convert_to_tensor(spectrogram, dtype=tf.float32)\n",
    "\n",
    "audio_df.insert(2, 'audio_tensor', audio_df['audio_path'].apply(spectrogram_to_tensor))\n",
    "audio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d371a5-04a0-4457-bcac-af1c26f02d5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087f22f-9836-4ba2-9d71-f1465b900b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display column types\n",
    "\n",
    "audio_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2817b8-d0b4-4fcd-8036-6acd22efdaf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display some audio files and their spectrogram\n",
    "\n",
    "audio_sample = audio_df.sample(5)\n",
    "\n",
    "for index, row in audio_sample.iterrows():\n",
    "    display(Audio(row['audio_path']))\n",
    "    y, sr = librosa.load(row['audio_path'])\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    librosa.display.specshow(librosa.power_to_db(spectrogram, ref=np.max), sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram (Label: ' + row['audio_label'] + ', Speaker: ' + row['speaker_label'] + ')')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f06bf5-2636-42ae-8a27-508468b9ff49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display all unique tensor shapes and their counts\n",
    "\n",
    "shapes = audio_df['audio_tensor'].apply(lambda x: x.shape)\n",
    "shapes_tuples = shapes.apply(tuple)\n",
    "unique_shape_counts = shapes_tuples.value_counts()\n",
    "\n",
    "print('Unique Shapes:')\n",
    "\n",
    "for shape, count in unique_shape_counts.items():\n",
    "    print(f'Shape: {shape}, Count: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0385d59-93e5-46b5-8e61-a7a513796034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resize the spectrogram tensors to same shapes\n",
    "\n",
    "def resize_spectrogram(tensor, target_shape=(128, 32)):\n",
    "    tensor = tf.expand_dims(tensor, axis=-1)\n",
    "    padded_tensor = tf.image.resize_with_crop_or_pad(tensor, target_shape[0], target_shape[1])\n",
    "    return padded_tensor.numpy()\n",
    "\n",
    "X = audio_df['audio_tensor'].apply(resize_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192ade8-2e23-4881-bc4b-c0455333ec6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display fixed tensor shapes\n",
    "\n",
    "fixed_shapes = X.apply(lambda x: x.shape)\n",
    "fixed_shapes_tuples = fixed_shapes.apply(tuple)\n",
    "fixed_shape_counts = fixed_shapes_tuples.value_counts()\n",
    "\n",
    "print('Unique Shapes:')\n",
    "\n",
    "for shape, count in fixed_shape_counts.items():\n",
    "    print(f'Shape: {shape}, Count: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59ecfa-8d11-4e24-9b79-309e15a4c5d8",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1055c-984c-44b8-a138-060e583378a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "X = np.stack(X)\n",
    "y = audio_df['audio_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "print('Size of dataset:', X.shape)\n",
    "print('Size of training set:', X_train.shape)\n",
    "print('Size of testing set:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f69c9a-a3cc-4d4a-8345-fcd17d970ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model architecture\n",
    "\n",
    "def AudioCNN(input_shape):\n",
    "    model = models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016ff2f-e4a3-4df1-8b21-0900ff341a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CNN model\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "model = AudioCNN(input_shape)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fd805-9aa3-4b9d-b421-f941a3c4b188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87694073-016e-47a6-b1b5-d5bfa6294e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display correct and incorrect predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "correct_idxs = np.where(predicted_labels == y_test)[0]\n",
    "incorrect_idxs = np.where(predicted_labels != y_test)[0]\n",
    "\n",
    "correct_labels = y_test.iloc[correct_idxs]\n",
    "print('Correct labels:', len(correct_labels))\n",
    "print(correct_labels)\n",
    "\n",
    "incorrect_labels = y_test.iloc[incorrect_idxs]\n",
    "print('Incorrect labels:', len(incorrect_labels))\n",
    "print(incorrect_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948bea6-ed57-4236-bb30-13420417064f",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e01967-89cf-41b7-a3fb-bf9025a4ec5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.save('audio_detector_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
