{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression generally have the form of $Y_{i} = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + ...$ <br>\n",
    "There are several ways to find the coefficients of the regression: <br>\n",
    "1. Linear Algebra: $\\hat{\\theta} = (X^{T}X)^{-1}X^{T}Y$ (When X is invertible) <br>\n",
    "2. Gradient Descent: In this case, we need to write out the loss function and try to minimize the loss. <br>\n",
    "$\\hspace{30mm}$ $F(x)$ = Loss Function = SE = $ \\sum^{n}_{i=1} (Y_{i} - \\hat{Y_{i}})^{2}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, alpha=1e-10 , num_iter=10000, early_stop=1e-50, intercept=True, init_weight=None): \n",
    "        \"\"\"\n",
    "        Some initializations, if neccesary\n",
    "        \n",
    "        attributes: \n",
    "            alpha: Learning Rate, default 1e-10\n",
    "            num_iter: Number of Iterations to update coefficient with training data\n",
    "            early_stop: Constant control early_stop.\n",
    "            intercept: Bool, If we are going to fit a intercept, default True.\n",
    "            init_weight: Matrix (n x 1), input init_weight for testing.\n",
    "                    \n",
    "        TODO: 1. Initialize all variables needed.\n",
    "        \"\"\"\n",
    "        self.model_name = 'Linear Regression'\n",
    "        self.alpha = alpha\n",
    "        self.num_iter = num_iter\n",
    "        self.early_stop = early_stop\n",
    "        self.intercept = intercept\n",
    "        self.init_weight = init_weight\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Save the datasets in our model, and perform gradient descent.\n",
    "        \n",
    "        Parameter:\n",
    "            X_train: Matrix or 2-D array. Input feature matrix.\n",
    "            Y_train: Matrix or 2-D array. Input target value.\n",
    "            \n",
    "        TODO: 2. If we are going to fit the intercept, add a col with all 1's to the first column. (hint: np.hstack, np.ones)\n",
    "              3. Initilaize our coef with uniform from [-1, 1] with the number of col in training set.\n",
    "              4. Call the gradient_descent function to train.\n",
    "        \"\"\"\n",
    "        self.X = np.asmatrix(X_train)\n",
    "        self.y = np.asmatrix(y_train).T\n",
    "        self.num_samples, self.num_features = self.X.shape\n",
    "        self.coef = self.init_weight\n",
    "        \n",
    "        if self.intercept:\n",
    "            ones = np.ones(shape=(self.X.shape[0], 1))\n",
    "            self.X = np.hstack((ones, X))\n",
    "            self.num_features += 1\n",
    "\n",
    "        if self.init_weight is None:\n",
    "            self.coef = np.random.uniform(-1, 1, (self.num_features, 1))\n",
    "\n",
    "        self.gradient_descent()\n",
    "        \n",
    "    def gradient(self):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the gradient respect to coefficient.\n",
    "        \n",
    "        TODO: 5. Think about the matrix format of the gradient of the loss function.\n",
    "        \"\"\"\n",
    "        y_pred = np.dot(self.X, self.coef)\n",
    "        error = self.y - y_pred\n",
    "        self.grad_coef = (-2 / len(self.X)) * self.X.T * error\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "        Training function\n",
    "        \n",
    "        TODO: 6. Calculate the loss with current coefficients.\n",
    "              7. Update the temp_coef with learning rate and gradient.\n",
    "              8. Calculate the loss with temp_coef.\n",
    "              9. Implement the self adeptive learning rate. \n",
    "                  a. If current error is less than previous error, increase learning rate by a factor 1.3. \n",
    "                     And update coef, with temp_coef.\n",
    "                  b. If previous error is less than current error, decrease learning rate by a factor of 0.9.\n",
    "                     Don't update coef.\n",
    "              10. Add the loss to loss list we create.\n",
    "        \"\"\"\n",
    "        self.loss = []\n",
    "        y_pred = np.dot(self.X, self.coef)\n",
    "        pre_error = float(np.sum(np.power(self.y - y_pred, 2)) / len(self.X))\n",
    "        self.loss.append(pre_error)\n",
    "        \n",
    "        for i in range(self.num_iter):                \n",
    "            self.gradient()\n",
    "\n",
    "            temp_coef = self.coef - self.alpha * self.grad_coef\n",
    "            self.coef, old_coef = temp_coef, self.coef.copy()\n",
    "            y_pred = np.dot(self.X, self.coef)\n",
    "            current_error = float(np.sum(np.power(self.y - y_pred, 2)) / len(self.X))\n",
    "            \n",
    "            # This is the early stop, don't modify fllowing three lines.\n",
    "            if (abs(pre_error - current_error) < self.early_stop) | (abs(abs(pre_error - current_error) / pre_error) < self.early_stop):\n",
    "                self.coef = temp_coef\n",
    "                return self\n",
    "            \n",
    "            if current_error <= pre_error:\n",
    "                self.alpha *= 1.3\n",
    "            else:\n",
    "                self.alpha *= 0.9\n",
    "                self.coef = old_coef\n",
    "                current_error = pre_error\n",
    "                \n",
    "            self.loss.append(current_error)\n",
    "            pre_error = current_error\n",
    "\n",
    "            # Print weights and loss information\n",
    "            # if i % 10000 == 0:\n",
    "            #     print('Iteration: ' +  str(i))\n",
    "            #     print('Coef: '+ str(self.coef))\n",
    "            #     print('Loss: ' + str(current_error))            \n",
    "        return self\n",
    "    \n",
    "    def ind_predict(self, x: list):\n",
    "        \"\"\"\n",
    "        Predict the value based on its feature vector x.\n",
    "\n",
    "        Parameter:\n",
    "        x: Matrix, array or list. Input feature point.\n",
    "        \n",
    "        Return:\n",
    "            result: prediction of given data point\n",
    "\n",
    "        TODO: 11. Implement the prediction function\n",
    "        \"\"\"\n",
    "        x = np.asmatrix(x)       \n",
    "        result = float(x * self.coef)\n",
    "        return result\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "        Each testing instance is a feature vector. \n",
    "        \n",
    "        Parameter:\n",
    "        X: Matrix, array or list. Input feature point.\n",
    "        \n",
    "        Return:\n",
    "            ret: prediction of given data matrix\n",
    "  \n",
    "        TODO: 12. Make sure add the 1's column like we did to add intercept.\n",
    "              13. Revise the following for-loop to call ind_predict to get predictions.\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        X = np.asmatrix(X)\n",
    "        if self.intercept:\n",
    "            ones = np.ones(shape=(X.shape[0], 1))\n",
    "            X = np.hstack((ones, X))\n",
    "            \n",
    "        for i in range(X.shape[0]):\n",
    "            ret.append(self.ind_predict(X[i]))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normaliz(lst):\n",
    "    \"\"\"\n",
    "    Helper function for normalize for faster training.\n",
    "    \"\"\"\n",
    "    maximum = np.max(lst)\n",
    "    minimum = np.min(lst)\n",
    "\n",
    "    return (lst - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate some easy data for testing. We should fit a line with, $Y = 30 * X + 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(np.asmatrix(np.arange(1, 1000, 5)).T)\n",
    "y = np.array((30 * X)).flatten() + 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do **NOT** modify the following line, just run it when you are done. You can also try different initialization, you will notice different coef at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = Linear_Regression(alpha=1, num_iter=10000000, init_weight=np.asmatrix([15,25]).T)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of iteration increase, you should notice the coeficient converges to [20, 30]. It maybe very slow update. Feel free to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[19.93498063],\n",
       "        [30.00009746]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pr/b_ytjflj1fqffyn22t_b8x_m0000gn/T/ipykernel_3386/1245635744.py:121: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  result = float(x * self.coef)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   49.93507809,   199.93556539,   349.93605269,   499.93653999,\n",
       "         649.93702729,   799.9375146 ,   949.9380019 ,  1099.9384892 ,\n",
       "        1249.9389765 ,  1399.9394638 ,  1549.9399511 ,  1699.94043841,\n",
       "        1849.94092571,  1999.94141301,  2149.94190031,  2299.94238761,\n",
       "        2449.94287492,  2599.94336222,  2749.94384952,  2899.94433682,\n",
       "        3049.94482412,  3199.94531142,  3349.94579873,  3499.94628603,\n",
       "        3649.94677333,  3799.94726063,  3949.94774793,  4099.94823524,\n",
       "        4249.94872254,  4399.94920984,  4549.94969714,  4699.95018444,\n",
       "        4849.95067174,  4999.95115905,  5149.95164635,  5299.95213365,\n",
       "        5449.95262095,  5599.95310825,  5749.95359556,  5899.95408286,\n",
       "        6049.95457016,  6199.95505746,  6349.95554476,  6499.95603206,\n",
       "        6649.95651937,  6799.95700667,  6949.95749397,  7099.95798127,\n",
       "        7249.95846857,  7399.95895588,  7549.95944318,  7699.95993048,\n",
       "        7849.96041778,  7999.96090508,  8149.96139238,  8299.96187969,\n",
       "        8449.96236699,  8599.96285429,  8749.96334159,  8899.96382889,\n",
       "        9049.9643162 ,  9199.9648035 ,  9349.9652908 ,  9499.9657781 ,\n",
       "        9649.9662654 ,  9799.9667527 ,  9949.96724001, 10099.96772731,\n",
       "       10249.96821461, 10399.96870191, 10549.96918921, 10699.96967652,\n",
       "       10849.97016382, 10999.97065112, 11149.97113842, 11299.97162572,\n",
       "       11449.97211302, 11599.97260033, 11749.97308763, 11899.97357493,\n",
       "       12049.97406223, 12199.97454953, 12349.97503684, 12499.97552414,\n",
       "       12649.97601144, 12799.97649874, 12949.97698604, 13099.97747334,\n",
       "       13249.97796065, 13399.97844795, 13549.97893525, 13699.97942255,\n",
       "       13849.97990985, 13999.98039716, 14149.98088446, 14299.98137176,\n",
       "       14449.98185906, 14599.98234636, 14749.98283366, 14899.98332097,\n",
       "       15049.98380827, 15199.98429557, 15349.98478287, 15499.98527017,\n",
       "       15649.98575748, 15799.98624478, 15949.98673208, 16099.98721938,\n",
       "       16249.98770668, 16399.98819398, 16549.98868129, 16699.98916859,\n",
       "       16849.98965589, 16999.99014319, 17149.99063049, 17299.9911178 ,\n",
       "       17449.9916051 , 17599.9920924 , 17749.9925797 , 17899.993067  ,\n",
       "       18049.9935543 , 18199.99404161, 18349.99452891, 18499.99501621,\n",
       "       18649.99550351, 18799.99599081, 18949.99647812, 19099.99696542,\n",
       "       19249.99745272, 19399.99794002, 19549.99842732, 19699.99891462,\n",
       "       19849.99940193, 19999.99988923, 20150.00037653, 20300.00086383,\n",
       "       20450.00135113, 20600.00183844, 20750.00232574, 20900.00281304,\n",
       "       21050.00330034, 21200.00378764, 21350.00427494, 21500.00476225,\n",
       "       21650.00524955, 21800.00573685, 21950.00622415, 22100.00671145,\n",
       "       22250.00719876, 22400.00768606, 22550.00817336, 22700.00866066,\n",
       "       22850.00914796, 23000.00963526, 23150.01012257, 23300.01060987,\n",
       "       23450.01109717, 23600.01158447, 23750.01207177, 23900.01255908,\n",
       "       24050.01304638, 24200.01353368, 24350.01402098, 24500.01450828,\n",
       "       24650.01499558, 24800.01548289, 24950.01597019, 25100.01645749,\n",
       "       25250.01694479, 25400.01743209, 25550.0179194 , 25700.0184067 ,\n",
       "       25850.018894  , 26000.0193813 , 26150.0198686 , 26300.0203559 ,\n",
       "       26450.02084321, 26600.02133051, 26750.02181781, 26900.02230511,\n",
       "       27050.02279241, 27200.02327972, 27350.02376702, 27500.02425432,\n",
       "       27650.02474162, 27800.02522892, 27950.02571622, 28100.02620353,\n",
       "       28250.02669083, 28400.02717813, 28550.02766543, 28700.02815273,\n",
       "       28850.02864004, 29000.02912734, 29150.02961464, 29300.03010194,\n",
       "       29450.03058924, 29600.03107654, 29750.03156385, 29900.03205115])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please try to normalize the X and fit again with normalized X. You should find something interesting. Also think about what you should do for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_norm, y_norm = min_max_normaliz(X), min_max_normaliz(y)\n",
    "clf.fit(X_norm,y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.19386068],\n",
       "        [0.00071316]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pr/b_ytjflj1fqffyn22t_b8x_m0000gn/T/ipykernel_3386/1245635744.py:121: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  result = float(x * self.coef)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.19457384, 0.19813963, 0.20170543, 0.20527122, 0.20883701,\n",
       "       0.2124028 , 0.21596859, 0.21953439, 0.22310018, 0.22666597,\n",
       "       0.23023176, 0.23379755, 0.23736334, 0.24092914, 0.24449493,\n",
       "       0.24806072, 0.25162651, 0.2551923 , 0.25875809, 0.26232389,\n",
       "       0.26588968, 0.26945547, 0.27302126, 0.27658705, 0.28015285,\n",
       "       0.28371864, 0.28728443, 0.29085022, 0.29441601, 0.2979818 ,\n",
       "       0.3015476 , 0.30511339, 0.30867918, 0.31224497, 0.31581076,\n",
       "       0.31937655, 0.32294235, 0.32650814, 0.33007393, 0.33363972,\n",
       "       0.33720551, 0.34077131, 0.3443371 , 0.34790289, 0.35146868,\n",
       "       0.35503447, 0.35860026, 0.36216606, 0.36573185, 0.36929764,\n",
       "       0.37286343, 0.37642922, 0.37999501, 0.38356081, 0.3871266 ,\n",
       "       0.39069239, 0.39425818, 0.39782397, 0.40138977, 0.40495556,\n",
       "       0.40852135, 0.41208714, 0.41565293, 0.41921872, 0.42278452,\n",
       "       0.42635031, 0.4299161 , 0.43348189, 0.43704768, 0.44061347,\n",
       "       0.44417927, 0.44774506, 0.45131085, 0.45487664, 0.45844243,\n",
       "       0.46200823, 0.46557402, 0.46913981, 0.4727056 , 0.47627139,\n",
       "       0.47983718, 0.48340298, 0.48696877, 0.49053456, 0.49410035,\n",
       "       0.49766614, 0.50123193, 0.50479773, 0.50836352, 0.51192931,\n",
       "       0.5154951 , 0.51906089, 0.52262668, 0.52619248, 0.52975827,\n",
       "       0.53332406, 0.53688985, 0.54045564, 0.54402144, 0.54758723,\n",
       "       0.55115302, 0.55471881, 0.5582846 , 0.56185039, 0.56541619,\n",
       "       0.56898198, 0.57254777, 0.57611356, 0.57967935, 0.58324514,\n",
       "       0.58681094, 0.59037673, 0.59394252, 0.59750831, 0.6010741 ,\n",
       "       0.6046399 , 0.60820569, 0.61177148, 0.61533727, 0.61890306,\n",
       "       0.62246885, 0.62603465, 0.62960044, 0.63316623, 0.63673202,\n",
       "       0.64029781, 0.6438636 , 0.6474294 , 0.65099519, 0.65456098,\n",
       "       0.65812677, 0.66169256, 0.66525836, 0.66882415, 0.67238994,\n",
       "       0.67595573, 0.67952152, 0.68308731, 0.68665311, 0.6902189 ,\n",
       "       0.69378469, 0.69735048, 0.70091627, 0.70448206, 0.70804786,\n",
       "       0.71161365, 0.71517944, 0.71874523, 0.72231102, 0.72587682,\n",
       "       0.72944261, 0.7330084 , 0.73657419, 0.74013998, 0.74370577,\n",
       "       0.74727157, 0.75083736, 0.75440315, 0.75796894, 0.76153473,\n",
       "       0.76510052, 0.76866632, 0.77223211, 0.7757979 , 0.77936369,\n",
       "       0.78292948, 0.78649528, 0.79006107, 0.79362686, 0.79719265,\n",
       "       0.80075844, 0.80432423, 0.80789003, 0.81145582, 0.81502161,\n",
       "       0.8185874 , 0.82215319, 0.82571898, 0.82928478, 0.83285057,\n",
       "       0.83641636, 0.83998215, 0.84354794, 0.84711374, 0.85067953,\n",
       "       0.85424532, 0.85781111, 0.8613769 , 0.86494269, 0.86850849,\n",
       "       0.87207428, 0.87564007, 0.87920586, 0.88277165, 0.88633744,\n",
       "       0.88990324, 0.89346903, 0.89703482, 0.90060061, 0.9041664 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try this with the wine dataset we use in HW1. Try fit this function to that dataset with same features. If you look closely to the updates of coefficients. What do you find? This could be mentioned in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "X = wine[['density', 'alcohol']]\n",
    "y = wine.quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [34.82170159  0.39144139]\n",
      "Squared Error: 800.6676988774332\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "# Squared Error with sklearn\n",
    "print(f'Coefficients: {lr.coef_}')\n",
    "print(f'Squared Error: {sum((lr.predict(X) - y)**2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice different coefficients, but the loss is very close to each other like 805. In your report, briefly discuss this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 5000000)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared Error: 805.3513209405593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pr/b_ytjflj1fqffyn22t_b8x_m0000gn/T/ipykernel_3386/1245635744.py:121: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  result = float(x * self.coef)\n"
     ]
    }
   ],
   "source": [
    "print(f'Squared Error: {sum((clf.predict(X) - y)**2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
